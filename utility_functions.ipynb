{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86a128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from binance.client import Client as bnb_client\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_binance_px(client, symbol, freq, start_ts = '2020-12-20'):\n",
    "    data = client.get_historical_klines(symbol, freq, start_ts)\n",
    "    columns = ['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume',\n",
    "               'num_trades', 'taker_base_volume', 'taker_quote_volume', 'ignore']\n",
    "    data = pd.DataFrame(data, columns = columns)\n",
    "    \n",
    "    # Convert from POSIX timestamp (number of millisecond since jan 1, 1970)\n",
    "    data['open_time'] = data['open_time'].map(lambda x: datetime.utcfromtimestamp(x/1000))\n",
    "    data['close_time'] = data['close_time'].map(lambda x: datetime.utcfromtimestamp(x/1000))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_price_data(univ, freq, should_fetch_cached_data, cached_data_file):\n",
    "    if should_fetch_cached_data:\n",
    "        px = pd.read_csv(cached_data_file)\n",
    "        date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "        px['open_time'] = px['open_time'].apply(lambda t:  datetime.strptime(t, date_format))\n",
    "        px.set_index('open_time', inplace=True)\n",
    "        return px\n",
    "    else:\n",
    "        client = bnb_client(tld='US')\n",
    "        px = {}\n",
    "        for x in univ:\n",
    "            print(f\"Downloading data for symbol {x}\")\n",
    "            data = get_binance_px(client, x, freq)\n",
    "            px[x] = data.set_index('open_time')['close']\n",
    "\n",
    "        px = pd.DataFrame(px).astype(float)\n",
    "        px.to_csv(cached_data_file)\n",
    "        return px\n",
    "\n",
    "\n",
    "# test_start_time_point must be in the index of df\n",
    "def get_train_test_data(df, test_start_time_point):\n",
    "    train_data = df.loc[:test_start_time_point].iloc[:-1]\n",
    "    test_data = df.loc[test_start_time_point:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def get_demeaned_normalized_signal(raw_signal):\n",
    "    signal_mean = raw_signal.mean(axis=1)\n",
    "    demeaned_signal = raw_signal.subtract(signal_mean, axis=0)\n",
    "    signal_sums = demeaned_signal.abs().sum(axis=1)\n",
    "    return demeaned_signal.divide(signal_sums, axis=0)\n",
    "\n",
    "\n",
    "def get_rank_demeaned_normalized_signal(raw_signal):\n",
    "    signal_rank = raw_signal.rank(axis=1)\n",
    "    signal_mean = raw_signal.rank(axis=1).mean(axis=1)\n",
    "    demeaned_signal = signal_rank.subtract(signal_mean, axis=0)\n",
    "    return demeaned_signal.divide(demeaned_signal.abs().sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "def get_gross_returns_and_net_returns(signal_weights, px):\n",
    "    asset_returns = px / px.shift() - 1\n",
    "    weighted_returns = signal_weights.shift() * asset_returns\n",
    "    gross_returns = weighted_returns.sum(axis=1)\n",
    "    turnover = (signal_weights.fillna(0) - signal_weights.shift().fillna(0)).abs().sum(axis=1)\n",
    "    tcost_bps = 20 # (commission + slippage)\n",
    "    net_returns = gross_returns.subtract(turnover * tcost_bps * 1e-4, fill_value = 0)\n",
    "    return gross_returns, net_returns\n",
    "\n",
    "\n",
    "def calculate_covariance_directly(ser_1, ser_2):\n",
    "    available_1 = ser_1.notna()\n",
    "    available_2 = ser_2.notna()\n",
    "    \n",
    "    common_1 = ser_1[available_1][available_2]\n",
    "    common_2 = ser_2[available_1][available_2]\n",
    "    \n",
    "    if common_1.shape[0] <= 1 or common_2.shape[0] <= 1:\n",
    "        return np.nan\n",
    "    \n",
    "    mean_1 = common_1.mean()\n",
    "    demeaned_1 = common_1 - mean_1\n",
    "    \n",
    "    mean_2 = common_2.mean()\n",
    "    demeaned_2 = common_2 - mean_2\n",
    "    \n",
    "    return (demeaned_1 * demeaned_2).sum() / (demeaned_1.shape[0] - 1)\n",
    "\n",
    "\n",
    "# Using pd.corrwith() yields a large number of hardware-related warnings so I implemented my \n",
    "# own version.\n",
    "def calculate_correlation_directly(ser_1, ser_2):\n",
    "    cov = calculate_covariance_directly(ser_1, ser_2)\n",
    "    \n",
    "    if cov == np.nan:\n",
    "        return np.nan\n",
    "\n",
    "    available_1 = ser_1.notna()\n",
    "    available_2 = ser_2.notna()\n",
    "    \n",
    "    common_1 = ser_1[available_1][available_2]\n",
    "    common_2 = ser_2[available_1][available_2]\n",
    "    \n",
    "    if len(common_1) <= 1 or len(common_2) <= 1:\n",
    "        return np.nan\n",
    "    \n",
    "    return cov / (common_1.std() * common_2.std())\n",
    "\n",
    "\n",
    "# returns pair in the form of (alpha, beta); nan values in dependent_series or independent_series are ignored;\n",
    "# neither can contain inf\n",
    "def get_alpha_beta_to_asset(dependent_series, independent_series):\n",
    "    cov = calculate_covariance_directly(dependent_series, independent_series)\n",
    "    \n",
    "    non_na_dependent_series = dependent_series[dependent_series.notna() & independent_series.notna()]\n",
    "    non_na_independent_series = independent_series[\n",
    "        dependent_series.notna() & independent_series.notna()]\n",
    "    \n",
    "    beta = cov / non_na_independent_series.var()\n",
    "    alpha = (non_na_dependent_series - non_na_independent_series * beta).mean()\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "def get_decorrelated_returns(strat_returns, benchmark_asset_returns):\n",
    "    non_na_period_strat_returns = strat_returns[strat_returns.notna() & benchmark_asset_returns.notna()]\n",
    "    non_na_period_benchmark_rets = benchmark_asset_returns[\n",
    "        strat_returns.notna() & benchmark_asset_returns.notna()]\n",
    "    _, beta = get_alpha_beta_to_asset(non_na_period_strat_returns, non_na_period_benchmark_rets)\n",
    "    return strat_returns - beta * benchmark_asset_returns\n",
    "\n",
    "\n",
    "def get_max_drawdown(net_returns):\n",
    "    cumulative_net_returns = net_returns.cumsum()\n",
    "    drawdowns = cumulative_net_returns / cumulative_net_returns.expanding(min_periods=1).max() - 1\n",
    "    return drawdowns[drawdowns != float('-inf')].min()\n",
    "\n",
    "\n",
    "def get_max_drawdown_duration(net_returns, hours_freq):\n",
    "    cumulative_net_returns = net_returns.cumsum()\n",
    "    \n",
    "    peak = cumulative_net_returns.expanding(min_periods=1).max()\n",
    "    \n",
    "    max_drawdown_duration = 0\n",
    "    current_drawdown_duration = 0\n",
    "    \n",
    "    for dt in cumulative_net_returns.index:\n",
    "        if cumulative_net_returns[dt] >= peak[dt]:\n",
    "            current_drawdown_duration = 0\n",
    "        else:\n",
    "            current_drawdown_duration += 1\n",
    "            max_drawdown_duration = max(max_drawdown_duration, current_drawdown_duration)\n",
    "    return max_drawdown_duration * hours_freq / 24\n",
    "\n",
    "\n",
    "# trade_hours_freq = 4, 8, 12, 24 (for 1 day), ...\n",
    "def get_strategy_stats(net_returns, trade_hours_freq, input_prices):\n",
    "    bitcoin_returns_over_period = input_prices['BTCUSDT'] / input_prices['BTCUSDT'].shift() - 1\n",
    "    \n",
    "    alpha, beta = get_alpha_beta_to_asset(net_returns.iloc[2:],\n",
    "                                          bitcoin_returns_over_period.iloc[2:])\n",
    "    decorrelated_returns = get_decorrelated_returns(net_returns, bitcoin_returns_over_period)\n",
    "    \n",
    "    res = {\n",
    "        \"avg returns\": net_returns.mean() * 24 / trade_hours_freq * 365,\n",
    "        \"decorrelated avg returns\": decorrelated_returns.mean() * 24 / trade_hours_freq * 365,\n",
    "        \"volatility\": net_returns.std() * np.sqrt(24 / trade_hours_freq * 365),\n",
    "        \"sharpe ratio\": net_returns.mean() / net_returns.std() * np.sqrt(24 / trade_hours_freq * 365),\n",
    "        \"decorrelated sharpe ratio\": decorrelated_returns.mean() / decorrelated_returns.std() * np.sqrt(24 / trade_hours_freq * 365),\n",
    "        \"max drawdown\": get_max_drawdown(net_returns),\n",
    "        \"max drawdown duration\": get_max_drawdown_duration(net_returns, trade_hours_freq),\n",
    "        \"alpha_BTC\": alpha,\n",
    "        \"beta_BTC\": beta,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "# For example, proportion_lo = 0.1 and proportion_hi = 0.1 makes the top and bottom 10% be the values at the\n",
    "# 90th and 10th percentiles, respectively.\n",
    "def get_winsorized_signal(raw_signal, proportion_lo, proportion_hi):\n",
    "    winsorized_signal = raw_signal.apply(lambda row: winsorize(\n",
    "        row, limits=[proportion_lo, proportion_hi]), axis=1, result_type='expand')\n",
    "    winsorized_signal.columns = raw_signal.columns\n",
    "    return winsorized_signal\n",
    "\n",
    "\n",
    "# For example, proportion_lo = 0.1 and proportion_hi = 0.1 makes the top and bottom 10% be removed.\n",
    "def get_truncated_signal(raw_signal, proportion_lo, proportion_hi):\n",
    "    quantile_lo = raw_signal.quantile(proportion_lo, axis=1)\n",
    "    mask_lo = raw_signal.lt(quantile_lo, axis=0)\n",
    "\n",
    "    quantile_hi = raw_signal.quantile(1-proportion_hi, axis=1)\n",
    "    mask_hi = raw_signal.gt(quantile_hi, axis=0)\n",
    "\n",
    "    return raw_signal.mask(mask_lo).mask(mask_hi)\n",
    "\n",
    "\n",
    "# For example, proportion_lo = 0.1 and proportion_hi = 0.1 makes the middle 80% of the data be removed.\n",
    "def get_rank_thresholded_signal(raw_signal, proportion_lo, proportion_hi):\n",
    "    quantile_lo = raw_signal.quantile(proportion_lo, axis=1)\n",
    "    mask_above = raw_signal.gt(quantile_lo, axis=0)\n",
    "\n",
    "    quantile_hi = raw_signal.quantile(1-proportion_hi, axis=1)\n",
    "    mask_below = raw_signal.lt(quantile_hi, axis=0)\n",
    "    \n",
    "    return raw_signal.mask(mask_above & mask_below)\n",
    "\n",
    "\n",
    "def get_inverse_cdf_standard_normal_signal(raw_signal):\n",
    "    ranked_signal = raw_signal.rank(axis=1)\n",
    "    num_non_na = ranked_signal.notna().astype(int).sum(axis=1)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        norm.ppf(ranked_signal.divide(num_non_na + 1, axis=0)),\n",
    "        columns=raw_signal.columns,\n",
    "        index=raw_signal.index)\n",
    "\n",
    "\n",
    "# px is expected to have data for every 4 hours\n",
    "def get_horizon_to_px_4h(px_df):\n",
    "    start_time = px_df.index[0]\n",
    "    end_time = px_df.index[-1]\n",
    "    return {\n",
    "        4: px_df,\n",
    "        8: px_df[px_df.index.hour % 8 == 0],\n",
    "        12: px_df[px_df.index.hour % 12 == 0],\n",
    "        24: px_df[px_df.index.hour == 0],\n",
    "        48: px_df[px_df.index.hour == 0].loc[pd.date_range(start=start_time, end=end_time, freq='2D')],\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_gross_and_net_cumulative_returns(gross_returns, net_returns):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    gross_returns.cumsum().plot(ax=axes[0], title='Cumulative Gross Returns', color='blue')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('Cumulative Return')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot the second Series on the second subplot (axes[1])\n",
    "    net_returns.cumsum().plot(ax=axes[1], title='Cumulative Net Returns', color='red')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Cumulative Return')\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Adjust layout to prevent overlapping titles/labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8827c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
